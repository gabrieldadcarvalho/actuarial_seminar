---
title: 'Credit Classification: Supervised Machine Learning'
author: "Gabriel D'assumpção de Carvalho"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    number_sections: true
    toc: true
    toc_depth: 2
    fig_caption: true
    dev: png
    keep_tex: true
always_allow_html: true
knitr:
  opts_chunk:
    dev: "png"
    dpi: 300
    fig.align: "center"
    fig.width: 7
    fig.height: 5
    echo: true
    warning: false
    message: false
    screenshot.force: true
header-includes: |
  \usepackage{amsmath}
  \usepackage{fancyhdr}
  \usepackage{listings}
  \usepackage{xcolor}
  \usepackage{fvextra}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{
      breaksymbolleft={},
      showspaces = false,
      showtabs = false,
      breaklines,
      commandchars=\\\{\}
    }
  \pagestyle{fancy}
  \fancyhead[L]{Credit Classification: Supervised Machine Learning}
  \fancyfoot[C]{Página \thepage}
  \fancyhead[C]{}
params:
  digits: 4
---

```{r, echo=FALSE}
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)
```

\newpage

# Libraries
```{r, warning=FALSE, message=FALSE}
# install.packages("ggplot2")
# install.packages("plotly")
# install.packages("caTools")
# install.packages("e1071")
# install.packages("class")

# Graphics
library(plotly)
library(ggplot2)
library(rpart.plot)

# Data Manipulation
library(caTools)
library(dplyr)

# Machine Learning
library(e1071) # For SVM
library(class) # For KNN
library(rpart) # For Decision Trees
library(caret) # For confusionMatrix

# Warning and message suppression
options(warn = -1)
suppressMessages(library(ggplot2))
suppressMessages(library(dplyr))
suppressMessages(library(caret))

set.seed(42)
```

# Introduction

# Exploratory Data Analysis (EDA)

## Data Import
```{r, warning=FALSE}
# Load the dataset
# Load the dataset
df <- read.csv("/home/gabrieldadcarvalho/github/actuarial_seminar/data/original.csv")[, -1]
print(head(df))
```

## Data Preprocessing

```{r, warning=FALSE}
# Check statistics of the dataset
summary(df)
```

```{r, warning=FALSE}
print(df$age[df$age <= 0])
```

Analyzing the statistics of the variables, we can see that the `age` variable has some tree NaN and negative values. Below i will converter the negative values to positive values, and for the NaN values, we discussed some imputation methods to handle these missing values.

```{r, warning=FALSE}
# Convert age for positive values
df$age <- abs(df$age)
print(summary(df$age))
```
We can see that the variable `age` has a tree NaN values. We can apply some imputation methods to handle these missing values. For example:

\begin{itemize}
  \item Mean imputation: Replace NaN values with the mean of the column.
  \item Median imputation: Replace NaN values with the median of the column.
  \item Linear Regression imputation: Use linear regression to predict missing values based on other variables.
  \item Regression imputation: Use regression models to predict missing values based on other variables.
  \item Interpolation: Use interpolation methods to estimate missing values based on surrounding data points.
  \item Exploratory Data Analysis (EDA): Analyze the data to understand the distribution and value intervals of the variables, and then apply one statistics to replace the NaN values.
\end{itemize}

# Exploratory Data Analysis (EDA)
```{r, warning=FALSE}
p1 <- plot_ly(df, x = ~age, type = "histogram", name = "Age") %>%
  layout(title = "Age", xaxis = list(title = "Age"), yaxis = list(title = "Count"), showlegend = FALSE)

p2 <- plot_ly(df, x = ~income, type = "histogram", name = "Income") %>%
  layout(title = "Income", xaxis = list(title = "Income"), yaxis = list(title = "Count"), showlegend = FALSE)

p3 <- plot_ly(df, x = ~loan, type = "histogram", name = "Loan") %>%
  layout(title = "Loan", xaxis = list(title = "Loan"), yaxis = list(title = "Count"), showlegend = FALSE)

subplot(p1, p2, p3,
  nrows = 2, margin = 0.07,
  titleX = TRUE, titleY = TRUE,
  shareX = FALSE, shareY = FALSE
) %>%
  layout(title = "Distribution of Explanatory Variables")
```

The `age` and `income` variables have a similar uniform distribution, while the `loan` variable has an asymmetric positive distribution. This affirmation is confirmed by the summary statistics of the dataset and the above plots.

```{r, warning=FALSE}
df %>%
  count(default) %>%
  plot_ly(
    x = ~ factor(default, levels = c(0, 1), labels = c("No (0)", "Yes (1)")),
    y = ~n,
    type = "bar",
    color = ~ factor(default),
    colors = c("#067906", "red"),
    name = "Default Variable"
  ) %>%
  layout(
    title = "Default",
    xaxis = list(title = "Default", type = "category"),
    yaxis = list(title = "Frequency"),
    showlegend = FALSE
  )
```

```{r, warning=FALSE}
corrPearson <- cor(df[, !names(df) %in% "default"], method = "pearson", use = "pairwise.complete.obs")
corrSpearman <- cor(df[, !names(df) %in% "default"], method = "spearman", use = "pairwise.complete.obs")
print(corrPearson)
print(corrSpearman)
```

## Linear Regression For Imputation data
For imputation, will be used the Linear Regression algorithm, which is a simple and effective methods for handling missing values. The Linear Regression algorithm works by finding the relationship between the target variable and the other features to predict the missing values.

```{r, warning=FALSE}
# Get the missing values in the 'age' column
ageNan <- df[is.na(df$age), ]
print(ageNan)

# Remove rows with NaN in 'age' for training and testing
dfNN <- df[!is.na(df$age), ]
print(summary(dfNN))
```

### Split the dataset into training and testing sets
```{r, warning=FALSE}
# Split the dataset into training and testing sets
split <- sample.split(dfNN$age, SplitRatio = 0.8)
train <- subset(dfNN, split == TRUE)
test <- subset(dfNN, split == FALSE)
```

### Normalize the numeric columns
```{r, warning=FALSE}
# Select numeric columns, excluding 'default'
numeric_cols <- sapply(train, is.numeric)
cols_for_stats <- names(train)[numeric_cols & names(train) != "default"]

# Calculate statistics only for the selected columns
means <- colMeans(train[, cols_for_stats], na.rm = TRUE)
sds <- apply(train[, cols_for_stats], 2, sd, na.rm = TRUE)

# Z-score normalization
for (c in colnames(train[(numeric_cols & names(train) != "default")])) {
  if (is.numeric(train[[c]])) {
    train[[c]] <- (train[[c]] - means[c]) / sds[c]
    test[[c]] <- (test[[c]] - means[c]) / sds[c]
    ageNan[[c]] <- (ageNan[[c]] - means[c]) / sds[c]
    df[[c]] <- (df[[c]] - means[c]) / sds[c]
  }
}

print(summary(train))
print(summary(test))
print(summary(ageNan))
```

### Train the Linear Regression model
```{r, warning=FALSE}
# Train a linear regression model to predict 'age'
model <- lm(age ~ ., data = train)
```
```{r, warning=FALSE}
print(summary(model))
```

### Test the model
```{r, warning=FALSE}
# Predict missing 'age' values in the test set
predictedAge <- predict(model, newdata = test)
```
```{r, warning=FALSE}
# Mean Squared Error (MSE) for the predictions
mse <- 0
for (i in 1:length(predictedAge)) {
  mse <- mse + (predictedAge[i] - test$age[i])^2
}

mse <- mse / length(predictedAge)
print(mse)
```

### Impute missing values in the original dataset
```{r, warning=FALSE}
# Impute missing values in the original dataset
ageNan$age <- predict(model, newdata = ageNan)

# Replace NaN values in the original dataset with the predicted values
df$age[is.na(df$age)] <- ageNan$age

# Reverse Z-score normalization for the imputed values
for (c in colnames(df[(numeric_cols & names(df) != "default")])) {
  if (is.numeric(df[[c]])) {
    df[[c]] <- (df[[c]] * sds[c]) + means[c]
    ageNan[[c]] <- (ageNan[[c]] * sds[c]) + means[c]
  }
}

print(head(ageNan))
print(summary(df))
```

# Predict Default

## Data Preparation
```{r, warning=FALSE}
split <- sample.split(df$default, SplitRatio = 0.8)
train <- subset(df, split == TRUE)
test <- subset(df, split == FALSE)

means <- colMeans(train[, cols_for_stats], na.rm = TRUE)
sds <- apply(train[, cols_for_stats], 2, sd, na.rm = TRUE)
```

```{r, warning=FALSE}
# Z-score normalization
for (c in colnames(train[(numeric_cols & names(train) != "default")])) {
  if (is.numeric(train[[c]])) {
    train[[c]] <- (train[[c]] - means[c]) / sds[c]
    test[[c]] <- (test[[c]] - means[c]) / sds[c]
  }
}
```

## Logistic Regression
```{r, warning=FALSE}
# Train a logistic regression model
logistic_model <- glm(default ~ ., data = train, family = binomial(link = "logit"))

print(summary(logistic_model))
```

```{r, warning=FALSE}
# Predict on the test set
logistic_pred <- predict(logistic_model, newdata = test, type = "response")
logistic_pred_class <- ifelse(logistic_pred > 0.5, 1, 0)

head(data.frame(Prediction = logistic_pred_class, Probability = logistic_pred))
```

```{r, warning=FALSE}
# Confusion matrix for logistic regression
confusionMatrix(as.factor(test$default), as.factor(logistic_pred_class), dnn = c("Reference", "Prediction"))
```

## Suport Vector Machine (SVM)
```{r, warning=FALSE}
# Train a Support Vector Machine (SVM) model
svm_model <- svm(default ~ ., data = train, kernel = "radial", cost = 1, gamma = 3, type = "C-classification", probability = TRUE)

summary(svm_model)
```

```{r svm-predict, message=FALSE, warning=FALSE}
# Predict on the test set using SVM
svm_pred <- predict(svm_model, newdata = test, probability = TRUE)
svm_probs <- attr(svm_pred, "probabilities")

head(svm_probs)

head(data.frame(Previsto = svm_pred, Prob_Classe_0 = svm_pred))
```

```{r svm-metrics, message=FALSE, warning=FALSE}
levels_ref <- c("0", "1")

reference <- factor(test$default, levels = levels_ref)
prediction <- factor(svm_pred, levels = levels_ref)

# Confusion matrix for SVM
confusionMatrix(reference, prediction, dnn = c("Reference", "Prediction"))
```

## Decision tree

### Data Preparation
```{r, warning=FALSE}
split <- sample.split(df$default, SplitRatio = 0.8)
train <- subset(df, split == TRUE)
test <- subset(df, split == FALSE)

means <- colMeans(train[, cols_for_stats], na.rm = TRUE)
sds <- apply(train[, cols_for_stats], 2, sd, na.rm = TRUE)

# Z-score normalization
for (c in colnames(train[(numeric_cols & names(train) != "default")])) {
  if (is.numeric(train[[c]])) {
    train[[c]] <- (train[[c]] - means[c]) / sds[c]
    test[[c]] <- (test[[c]] - means[c]) / sds[c]
  }
}
print(summary(train))
```

```{r, warnings=FALSE}
dt_model <- rpart(default ~ ., data = train, method = "class")
```

```{r, warning=FALSE}
rpart.plot(dt_model, type = 5, extra = 104, main = "Decision Tree for Default Prediction", box.palette = "RdYlGn", shadow.col = "gray")
```

### Predicting
```{r, warning=FALSE}
predictions <- predict(dt_model, newdata = test, type = "class")
```

```{r, warning=FALSE}
# Confusion matrix for Decision Tree
confusionMatrix(as.factor(test$default), as.factor(predictions), dnn = c("Reference", "Prediction"))
```