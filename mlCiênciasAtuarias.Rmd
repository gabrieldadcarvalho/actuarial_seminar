---
pdf_document:
  fig_caption: true
  number_sections: true
  dev: png
author: "Gabriel D'assumpção de Carvalho"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    number_sections: true
    toc: true         # Gera o sumário automaticamente
    toc_depth: 2      # Limita o sumário a dois níveis
  pdf_document: default
title: 'Credit Classification:   Machine Learning'
params:
  digits: 4
header-includes: |
  \usepackage{amsmath}
  \usepackage{fancyhdr}
  \usepackage{listings}
  \lstset{
    breaklines=true,
    basicstyle=\ttfamily\small,
    frame=single,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
    breakindent=10pt
  }
  \pagestyle{fancy}
  \fancyhead[L]{Resumo do Actuar}
  \fancyfoot[C]{Página \thepage}
  \fancyhead[C]{}
knitr:
  opts_chunk:
    screenshot.force: TRUE
---


\newpage

# Libraries
```{r, warning=FALSE, message=FALSE}
# install.packages("ggplot2")
# install.packages("plotly")
# install.packages("caTools")
# install.packages("e1071")
# install.packages("class")

# Graphics
library(plotly)
library(ggplot2)
library(rpart.plot)

# Data Manipulation
library(caTools)
library(dplyr)

# Machine Learning
library(e1071) # For SVM
library(class) # For KNN
library(rpart) # For Decision Trees
library(caret) # For confusionMatrix

# Warning and message suppression
options(warn = -1)
suppressMessages(library(ggplot2))
suppressMessages(library(dplyr))
suppressMessages(library(caret))

set.seed(42)
```

# Introduction

# Exploratory Data Analysis (EDA)

## Data Import
```{r, warning=FALSE}
# Load the dataset
df <- read.csv("/home/gabrieldadcarvalho/github/actuarial_seminar/data/original.csv")[, -1]
print(head(df))
```

## Data Preprocessing

```{r, warning=FALSE}
# Check statistics of the dataset
summary(df)
```

```{r, warning=FALSE}
print(df$age[df$age <= 0])
```

Analyzing the statistics of the variables, we can see that the `age` variable has some tree NaN and negative values. Below i will converter the negative values to positive values, and for the NaN values, we discussed some imputation methods to handle these missing values.

```{r, warning=FALSE}
# Convert age for positive values
df$age <- abs(df$age)
print(summary(df$age))
```
We can see that the variable `age` has a tree NaN values. We can apply some imputation methods to handle these missing values. For example:

\begin{itemize}
  \item Mean imputation: Replace NaN values with the mean of the column.
  \item Median imputation: Replace NaN values with the median of the column.
  \item Linear Regression imputation: Use linear regression to predict missing values based on other variables.
  \item Regression imputation: Use regression models to predict missing values based on other variables.
  \item Interpolation: Use interpolation methods to estimate missing values based on surrounding data points.
  \item Exploratory Data Analysis (EDA): Analyze the data to understand the distribution and value intervals of the variables, and then apply one statistics to replace the NaN values.
\end{itemize}

# Exploratory Data Analysis (EDA)
```{r, warning=FALSE}
p1 <- plot_ly(df, x = ~age, type = "histogram", name = "Age") %>%
  layout(title = "Age", xaxis = list(title = "Age"), yaxis = list(title = "Count"), showlegend = FALSE)

p2 <- plot_ly(df, x = ~income, type = "histogram", name = "Income") %>%
  layout(title = "Income", xaxis = list(title = "Income"), yaxis = list(title = "Count"), showlegend = FALSE)

p3 <- plot_ly(df, x = ~loan, type = "histogram", name = "Loan") %>%
  layout(title = "Loan", xaxis = list(title = "Loan"), yaxis = list(title = "Count"), showlegend = FALSE)

subplot(p1, p2, p3,
  nrows = 2, margin = 0.07,
  titleX = TRUE, titleY = TRUE,
  shareX = FALSE, shareY = FALSE
) %>%
  layout(title = "Distribution of Explanatory Variables")
```

The `age` and `income` variables have a similar uniform distribution, while the `loan` variable has an asymmetric positive distribution. This affirmation is confirmed by the summary statistics of the dataset and the above plots.

```{r, warning=FALSE}
df %>%
  count(default) %>%
  plot_ly(
    x = ~ factor(default, levels = c(0, 1), labels = c("No (0)", "Yes (1)")),
    y = ~n,
    type = "bar",
    color = ~ factor(default),
    colors = c("#067906", "red"),
    name = "Default Variable"
  ) %>%
  layout(
    title = "Default",
    xaxis = list(title = "Default", type = "category"),
    yaxis = list(title = "Frequency"),
    showlegend = FALSE
  )
```

```{r, warning=FALSE}
corrPearson <- cor(df[, !names(df) %in% "default"], method = "pearson", use = "pairwise.complete.obs")
corrSpearman <- cor(df[, !names(df) %in% "default"], method = "spearman", use = "pairwise.complete.obs")
print(corrPearson)
print(corrSpearman)
```

## Linear Regression For Imputation data
For imputation, will be used the Linear Regression algorithm, which is a simple and effective methods for handling missing values. The Linear Regression algorithm works by finding the relationship between the target variable and the other features to predict the missing values.

```{r, warning=FALSE}
# Get the missing values in the 'age' column
ageNan <- df[is.na(df$age), ]
print(ageNan)

# Remove rows with NaN in 'age' for training and testing
dfNN <- df[!is.na(df$age), ]
print(summary(dfNN))
```

### Split the dataset into training and testing sets
```{r, warning=FALSE}
# Split the dataset into training and testing sets
split <- sample.split(dfNN$age, SplitRatio = 0.8)
train <- subset(dfNN, split == TRUE)
test <- subset(dfNN, split == FALSE)
```

### Normalize the numeric columns
```{r, warning=FALSE}
# Select numeric columns, excluding 'default'
numeric_cols <- sapply(train, is.numeric)
cols_for_stats <- names(train)[numeric_cols & names(train) != "default"]

# Calculate statistics only for the selected columns
means <- colMeans(train[, cols_for_stats], na.rm = TRUE)
sds <- apply(train[, cols_for_stats], 2, sd, na.rm = TRUE)

# Z-score normalization
for (c in colnames(train[(numeric_cols & names(train) != "default")])) {
  if (is.numeric(train[[c]])) {
    train[[c]] <- (train[[c]] - means[c]) / sds[c]
    test[[c]] <- (test[[c]] - means[c]) / sds[c]
    ageNan[[c]] <- (ageNan[[c]] - means[c]) / sds[c]
    df[[c]] <- (df[[c]] - means[c]) / sds[c]
  }
}

print(summary(train))
print(summary(test))
print(summary(ageNan))
```

### Train the Linear Regression model
```{r, warning=FALSE}
# Train a linear regression model to predict 'age'
model <- lm(age ~ ., data = train)
```
```{r, warning=FALSE}
print(summary(model))
```

### Test the model
```{r, warning=FALSE}
# Predict missing 'age' values in the test set
predictedAge <- predict(model, newdata = test)
```
```{r, warning=FALSE}
# Mean Squared Error (MSE) for the predictions
mse <- 0
for (i in 1:length(predictedAge)) {
  mse = mse + (predictedAge[i] - test$age[i])^2
}

mse = mse / length(predictedAge)
print(mse)
```

### Impute missing values in the original dataset
```{r, warning=FALSE}
# Impute missing values in the original dataset
ageNan$age <- predict(model, newdata = ageNan)

# Replace NaN values in the original dataset with the predicted values
df$age[is.na(df$age)] <- ageNan$age

# Reverse Z-score normalization for the imputed values
for (c in colnames(df[(numeric_cols & names(df) != "default")])) {
  if (is.numeric(df[[c]])) {
    df[[c]] <- (df[[c]] * sds[c]) + means[c]
    ageNan[[c]] <- (ageNan[[c]] * sds[c]) + means[c]
  }
}

print(head(ageNan))
print(summary(df))
```

# Predict Default

## Data Preparation
```{r, warning=FALSE}
split <- sample.split(df$default, SplitRatio = 0.8)
train <- subset(df, split == TRUE)
test <- subset(df, split == FALSE)

means <- colMeans(train[, cols_for_stats], na.rm = TRUE)
sds <- apply(train[, cols_for_stats], 2, sd, na.rm = TRUE)
```

```{r, warning=FALSE}
# Z-score normalization
for (c in colnames(train[(numeric_cols & names(train) != "default")])) {
  if (is.numeric(train[[c]])) {
    train[[c]] <- (train[[c]] - means[c]) / sds[c]
    test[[c]] <- (test[[c]] - means[c]) / sds[c]
  }
}
```

## Logistic Regression
```{r, warning=FALSE}
# Train a logistic regression model
logistic_model <- glm(default ~ ., data = train, family = binomial(link = "logit"))

print(summary(logistic_model))
```

```{r, warning=FALSE}
# Predict on the test set
logistic_pred <- predict(logistic_model, newdata = test, type = "response")
logistic_pred_class <- ifelse(logistic_pred > 0.5, 1, 0)

head(data.frame(Prediction = logistic_pred_class, Probability = logistic_pred))
```

```{r, warning=FALSE}
# Confusion matrix for logistic regression
confusionMatrix(as.factor(test$default), as.factor(logistic_pred_class), dnn = c("Reference", "Prediction"))
```

## Suport Vector Machine (SVM)
```{r, warning=FALSE}
# Train a Support Vector Machine (SVM) model
svm_model <- svm(default ~ ., data = train, kernel = "radial", cost = 1, gamma = 3)

summary(svm_model)
```

```{r svm-predict, message=FALSE, warning=FALSE}
# Predict on the test set using SVM
svm_pred <- predict(svm_model, newdata = test)
svm_pred_class <- ifelse(svm_pred >= 0.5, 1, 0)

head(data.frame(Previsto = svm_pred_class, Prob = svm_pred))
```

```{r svm-metrics, message=FALSE, warning=FALSE}
levels_ref <- c("0", "1")

reference <- factor(test$default, levels = levels_ref)
prediction <- factor(svm_pred_class, levels = levels_ref)

# Confusion matrix for SVM
confusionMatrix(reference, prediction, dnn = c("Reference", "Prediction"))
```

## Decision tree

### Data Preparation
```{r, warning=FALSE}
split <- sample.split(df$default, SplitRatio = 0.8)
train <- subset(df, split == TRUE)
test <- subset(df, split == FALSE)

means <- colMeans(train[, cols_for_stats], na.rm = TRUE)
sds <- apply(train[, cols_for_stats], 2, sd, na.rm = TRUE)

trainT <- train["default"]
testT <- test["default"]

trainT$ageInterval <- cut(train$age, breaks = c(18, 40, 60, Inf), right = FALSE)
testT$ageInterval <- cut(test$age, breaks = c(18, 40, 60, Inf), right = FALSE)

# Menos intervalos para income
trainT$incomeInterval <- cut(train$income, breaks = c(20000, 50000, 100000, Inf), right = FALSE)
testT$incomeInterval <- cut(test$income, breaks = c(20000, 50000, 100000, Inf), right = FALSE)

# Menos intervalos para loan
trainT$loanInterval <- cut(train$loan, breaks = c(0, 5000, 10000, Inf), right = FALSE)
testT$loanInterval <- cut(test$loan, breaks = c(0, 5000, 10000, Inf), right = FALSE)


```

```{r, warning=FALSE}
ageClass <- table(trainT$ageInterval, trainT$default)
print(ageClass)

incomeClass <- table(trainT$incomeInterval, trainT$default)
print(incomeClass)

loanClass <- table(trainT$loanInterval, trainT$default)
print(loanClass)
```

```{r, warnings=FALSE}
dt_model <- rpart(default ~., data=train, method = "class")
```

```{r, warning=FALSE}
rpart.plot(dt_model, type=5, extra=104, main="Decision Tree for Default Prediction", box.palette = "RdYlGn", shadow.col = "gray")
```

### Predicting
```{r, warning=FALSE}
predictions <- predict(dt_model, newdata = test, type = "class")
```

```{r, warning=FALSE}
# Confusion matrix for Decision Tree
confusionMatrix(as.factor(test$default), as.factor(predictions), dnn = c("Reference", "Prediction"))
```
